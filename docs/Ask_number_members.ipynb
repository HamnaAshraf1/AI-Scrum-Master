{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code loads the LLaMA tokenizer and model from the Hugging Face library, tokenizes the input prompt, and generates a response using the model. It then decodes the model's output to readable text. Finally, it prints the generated response. This allows interaction with the LLaMA model to ask questions and get answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = LlamaTokenizer.from_pretrained('facebook/llama-7b')\n",
    "model = LlamaForCausalLM.from_pretrained('facebook/llama-7b')\n",
    "\n",
    "def ask_llama(prompt):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate the response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Define your prompt to ask for the number of members\n",
    "prompt = \"How many members are there?\"\n",
    "\n",
    "# Get the response from the LLaMA model\n",
    "response = ask_llama(prompt)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
